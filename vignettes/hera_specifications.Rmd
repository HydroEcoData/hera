---
title: "Request for Comment: Shared Design Principles for Creating and Deploying Classification Methods"
date: "`r Sys.Date()`"
pkgdown:
  as_is: yes
output: 
  rmarkdown::html_vignette:
    number_sections: yes
    toc: yes
vignette: >
  %\VignetteIndexEntry{Request for Comment: Shared Design Principles for Creating and Deploying Classification Methods}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}  
editor_options: 
  chunk_output_type: console 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(comment = "")
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

library(tidyverse)
library(hera)
library(darleq3)
library(fcs2)
library(rict)

```

```{r results="asis", echo=FALSE}
# directly adding css to output html without ruining css style https://stackoverflow.com/questions/29291633/adding-custom-css-tags-to-an-rmarkdown-html-document
cat("
<style>
img {
border: 0px;
outline: 0 ;
}
</style>
")
```
WORK IN PROGRESS - Drafting

![](images/heraverse_logo_2.png){ width=50% }

WARNING: Blue sky thinking ahead

Keywords: collection, modelling, prediction, classification, forecast, scenario

# Summary

We propose all new tool development share a common set of design principles,
interfaces and data structures.

Specifically, we propose an official collection of R packages designed to
provide collaborative workflow for building and using classification tools. In
turn, these packages will be integrated into a single package called 'hera'.
This provides a common interface to run classification. We expect this process
will facilitate code re-use, faster integration and knowledge exchange
between method developers and practitioners.

# Motivation

UKTAG has guided the development of an impressive range of classification tools.
This has involved many developers, researchers and experts dedicating their time
and effort to creating tools to better understand pressures on the environment.
We are confident that there are many future opportunities for collaboration and
tool development in response to changing environmental pressures and improving
scientific understanding. As access to modelling tools become more routine, we
expect a proliferation of models and indices in the years ahead. For instance,
new tools for diagnosing pressures, updates to existing tools and catchment
scale planning. To effectively combine multiple model and tools into an
integrate system to understand the environment we propose a they share a common
design philosophy to aid integration and management.

# Key Ideas

The aims of a shared design philosophy for WFD classification R packages include:
  
- Create a single user interfaces, similar data formats and operating procedures.
- Allow outputs from multiple tools to be quickly generated and combined together.
- Shorten the time between development and integration into Agencies systems.
- Clearer path for researcher engagement and model development.
- Make it easier to share and re-use code between tools for common functions.
- Share data quality standards and data validation code. 
- Apply similar approaches to code review, testing and documentation.

## Preparing for future

In the next 10 years...
  
We assume it is likely that our aquatic ecological models will be subsumed into
a more large scale environmental and climate models. These 'total environment'
models may for instance use climate change models to forecast impact on
invertebrates, water-use models to predict the impact on fish or spatial
planning tools the impact on nutrient levels. The outputs will be used across
disciples both for RBMP and beyond for flood management, biodiversity
improvements, carbon sequestering etc. Allowing multi-discipline assessment of
impacts and trade-offs for each planning scenario and proposed measures -
ensuring well-informed decisions-making.

All ecological data along with chemistry, climate, meteorological, geological
will be freely and easily accessible. We assume agencies will upload fish
counter data, plant DNA or aerial imagery etc, which will be ingested into the
'lake' of environmental data.

To take step towards this vision, the underlying design of a models and tools
must be modular and easy to connect and integrate in a variety of ways.

# Detailed design

To aid collaboration and to response to the changing environmentally pressures,
we propose creating a joint collections of packages to share understanding on the
environment while providing the software infrastructure to lighten the burden of
more mundane tasks involved in maintaining and deploying new models and
interfaces. 

This is influenced by the work of climate change research and weather
forecasting communities. And on-going work in the R community such as [ropenSci](https://ropensci.org/).

## Shared Packages

Currently we have a number of model packages shared on github:
  
1. [darleq3](https://github.com/nsj3/darleq3) - Phytobenthos  
2. [fsc2](https://github.com/aquaMetrics/fcs2) - Scottish river fish  
3. [rict](https://github.com/aquaMetrics/rict) - General invertebrate classification  
  
We propose these tools become part of official collection of packages and
overtime, integrate more fully with each other.

## Hera

Here we introduce a prototype R package called
[hera](https://github.com/ecodata1/hera). The key idea, is `hera` provides a
common interface for existing WFD packages and future developments. This is
achieved through a shared set of functions required to run and report
classification. It builds on the best practice idea of how to [run many
models](https://r4ds.had.co.nz/many-models.html) simultaneously in R while
keeping the input and output data formats simple and homogeneous. We explain each
function in detail below.

### Key steps

These key steps allows functions to be developed independently. Much of the code
and rules can be re-used for future tool development.

The examples below provide a taste of how this RFC could be implemented. Keep in mind,
they are not full, complete or accurate. The data structure, naming and details
could all change.

### Validate

Firstly, validate predictors and observations are correct and within the bounds of the model:

```{r, echo=TRUE}
set.seed <- 42
demo_data <- hera::demo_data
validations <- validation(demo_data)
head(validations, 5)
```

### Calculate Indices

We run validated data through the `indices` function, calculating observed indices scores as required for each sample:


```{r, echo=TRUE}
 
 indices(demo_data) %>% 
 group_by(analysis_repname) %>% 
 slice_sample(n = 4) %>% 
 select(sample_number, indices)

```

Note, the invert indices are not calculated, they have been pre-calculated and are in the demo_data from the start. 

### Predict 

We then predict reference scores for each sample:

```{r, echo=TRUE}
 prediction(demo_data[1:2200, ]) %>% 
 group_by(analysis_repname) %>% 
 slice_sample(n = 4) %>% 
 select(sample_number, prediction) 

```

See documentation [`prediction()`](https://ecodata1.github.io/hera/reference/prediction.html)
for more information

### Classify

Based on the predicted and observed indices - we classify each sample:

```{r, echo=TRUE}

 classification(demo_data) %>% 
 group_by(analysis_repname) %>% 
 slice_sample(n = 4) %>% 
 select(sample_number, classification)

```

Note, classification results for inverts are missing - this has not be implemented yet.

### Aggegate

We now aggregate the samples by season, year or multi-year as required:

```

aggregate <- aggregations(demo_data, c("year","season","waterbody"))
head(class)

```

### Report

We now run a report assesses samples from the same location for consistency.
Note, that no report for fish is produced, the necessary adjustment/interpretation
parameters have not been created for fish. This demonstrates that it is step by
step process. Not all tools/models will have all the features development if not
required or if not prioritized:

```
compare_report <- compare(new_data, old_data)
compare_report

```

We now run a report assesses two samples assess a discharge (up and downstream):  

```
compare_report <- compare(site_one, site_two)
compare_report

```

### Bringing it all together

For the most part we don't expect users to go through each of these steps. But for developers and researchers it is useful to think about classification within this framework. For the majority of end users, agency staff or consultants, they can open a the GUI `hera_app()` (or hosted website). However advanced users can use the `hera` function to wrap all these steps together from example `hera(hera::demo_data)`.

Furthermore, agencies can integrate these functions into their systems using web services. Please see the demo service that has been hosted to demonstrate this option. 

## Sharing

Hera allows multiple ecological elements to be assessed through the same
interface. But not just the interface is shared. Other areas of the
infrastructure are shared including:

- Reporting and comparison tools can also be shared between multiple
elements. 
- Validation checks and other universally required mechanisms
are shared and easily configurable for new models/methods.
- Testing infrastructure. 
- Confidence of class and data suitability algorithms. 

## Organisation

All UKTAG sub-groups and their nominated leads in the devolved agencies would
contribute new method develops and tools to the shared collection of packages.
Where tools are agency specific, these could also make use of the platform if
required.

As agencies commission new tools to be developed, researchers can upload their
predictive variables, reference data and models into a central repo for easier
collaboration.

## Lots of datasets - one underlying data structure?

Ecological modelling relies on sampling. The samples come in a range of forms
from points, transects, images, grabs, DNA etc. But the general feature of
modelling is based on being able to predict what we expect to find from whatever
sampling technique we deploy. The sample is the fundamental observation which we
compare against our prediction. The samples are discreet and independent, either
observed instantaneously or perhaps over a few minutes or hours (where dynamic
changes are not significant).
  
Multiple samples can be aggregated to smooth variance but the sample still
remains the fundamental building block. The sample could be a single pixel from
an aerial image or a salmon moving through a fish counter. We still make
predictions of what we expect this sample to be like even if the true picture
only emerges after several samples are aggregated or compared.

## What does this look like?

This demo dataset contains information on diatoms, fish and inverts:

```
library(hera)
str(demo_data)

```

Below is an example of diatom records, invert data and river flow in a shared
input format.
  
They share some reference/book keeping variables but not all. Ultimately, only
one reference is needed which is a unique sample id. The other reference
variables can be 'nested' or in others words there can be as many or a few as
you like. These nested values could be sample type, collector, instrument
details etc. Or variables later used for aggregation such as water body, river,
geographic area etc. For example:

```{r}
data <- utils::read.csv(system.file("extdat",
                                    "test-data.csv",
                                    package = "hera"
))


knitr::kable(data.frame("sample_id" = 192342,
                        "nested meta data" = names(data[, 2:6]),
                        "question" = "whpt_aspt_abundance",
                        "response" = 6.348,
                        check.names = FALSE))

```

## Predictors

Predictive variables such as temperature, altitude, slope etc are also metadata
can can be nested, as they are the same for each sample.

## Observations 

The observation comes in two parts, the unique name/id for what you are
observing and the value associated with it. For clarity these variables are
called 'question' and 'response'. The question could be "Taxon name?" and the
response "Brown Trout". However, the question must be unique. As 'Taxon name?'
will be different depending on what survey is being undertaken. So question_id
and response_id are given to allow these to be unique with metadata to provide a
more readable format.
  
UUID are used to provide unique IDs for this variables. 

```{r}

 data <- nest(data, meta_variables = c(2:6, 9:26))
# data <- nest(meta_variables = c(2:6))
knitr::kable(data[1:2,])

```

## Data input

Data input is through mobile or internet connected devices. The question_id and
related meta data is configured. Manual or automated data can be collected.
As new WFD developments and updates requirements are identified, the lead
contacts from the agencies and method developers are 'on-boarded' to explain the
expectations and examples of building collaborative framework of packages. Where
skill develop is required further training can be provided, or additional
external or internal support from the agency commissioning the work.
A workshop for lead data experts / R coders from each agency delivers
institutional knowledge on how internally developed tools will fit with the
design philosophy and expectations of how external researchers will collaborate
on building new tools.

## Model platform

There is no prescribed modelling program or software. Researchers can download
the data provided and use any software they desire - as long as it has an API

Alternatively, if researchers can't provide an api for others. The
recommendation is to use R - which integrates more directly into the pipeline.

Once modelling is completed, the model object is saved and deployed. Any
existing or future data collected using the platform will be run through the
model at the sample level.

Researchers can then build tools to display and aggregate the sample level
results as required (Waterbody, Year, Catchment etc). Where it would be possible
to share techniques for producing Confidence of Class, assessment of data
suitability and adjustment factors etc.


# How we Teach This

As new WFD developments and updates requirements are identified, the lead
contacts from the agencies and method developers are 'on-boarded' to demonstrate
the design principles and collaborative framework of packages. Where skill
development is required further training can be provided, or additional external
or internal support from the agency commissioning the work.
  
A workshop for lead data experts / R coders from each agency delivers
institutional knowledge on how internally developed tools will fit with the
shared design philosophy as well as seeting expectations for colloaboration with
external researchers.

# Alternatives



# Unresolved questions

- Should all reference data be combined into a single repository / web service?
- Should all predictor data be combined into a single repository / web service?

# Appendix


## Input formats

### FCS2 tool 

Demo input data format (truncated) and full list of column names
```{r}
test <- head(demo_data[, 1:5], 4)
test$... <- "..."
test
row.names(test) <- NULL
names(demo_data)
```

### Darleq tool

Input data for DARLEQ3 tool is a list of dataframes. Here's an example of input
data format (truncated) and full list of column names

```{r}

file <- system.file("extdata/DARLEQ2TestData.xlsx", package="darleq3")
data <- read_DARLEQ(file, "Rivers TDI Test Data")
test <- data$diatom_data[1:4, 1:8]
test$... <- "..."
test
names(data$diatom_data)
names(data$header)
```

### RICT

Here's an example of input data format (truncated) and full list of column names

```{r}
test <- rict::demo_observed_values[1:4, 1:8]
test$... <- "..."
test
names(demo_observed_values)
```

## Input formats

### FCS2 example

```{r, include=FALSE}
results <- calcClassScot(data = fcs2::demo_data)
```

```{r}
test <- results[1:4, 1:6]
test$... <- "..."
test
names(results)
```

### RICT example

```{r, warning=F, message=F}
test <- rict(demo_ni_observed_values)
example <- head(test[1: 6], 4)
example$... <- "..."
example
names(test)
```

### Darleq data

(list of dataframes)

```{r}
fn <- system.file("extdata/DARLEQ2TestData.xlsx", package="darleq3")
d <- read_DARLEQ(fn, "Rivers TDI Test Data")
results <- calc_Metric_EQR(d, metrics=c("TDI4", "TDI5LM"))
head(results$TDI5LM$EQR[, 9:13])
head(results$TDI5LM$Uncertainty[, 9:13])
head(results$TDI5LM$Metric)
head(results$TDI5LM$Job_Summary, 4)
```




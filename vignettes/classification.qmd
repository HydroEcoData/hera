---
title: "assessment-checking"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

# TODO

-   Grab all the data ✓
-   Compare changes in real classification between years?
-   Difficult because sites not always sampled every year.
-   Need a solution.
-   Compare to changes between new tools and old for single year(s)?
-   95% changes in diatom classification? Needs more checking - possibility alkalinity 75 override issues?
-   Check rict using year average predictors
-   Tested sample_id = 2755381, slight difference in EQR. Maybe due to difference in Akalinity?
-   Use tests from sepaData package
-   Need to use mean_alkalinity( n = 10) to get previously used Alkalinity ✓
-   Check one site against RICT3 output?
-   Need solution for finding water body IDs for location for invert sites. No contributing location? - Use NMP spreadsheet?
-   Check results from darleq2 outputs against NEMS? Run sample through DARLEQ3 and hera?
-   Have a look at suitability codes
-   Have a look at warnings and fails?
-   Have a look at excluded samples (missing Alk for Diatoms etc)
-   Use dynamic predictive variables
-   Test one sample from beginning to end??
-   Test 8175 first? Using unit tests? OKAY! ✓
-   Unit test for mean_alkalinity
-   Unit test for RICT
-   Unit test for DARLEQ
-   Get all alkalinity from lims ✓
-   Get alkalinity from recovered data - all data? ✓
-   Join lims + recovered alk - 'hera' format? ✓
-   Update sampling point table
-   Join chemistry with Ecology data ✓
-   Query alkalinity data using alt_site - get mean_alkalinty ✓
-   Check missing grid_reference from recovered data? Join location table?
-   Add suitability code
-   Classify
-   Join with 2020 classification?
-   Sewage fungus override - manual check?
-   Missing NGR in recovered data?
-   Convert() function does a lot of tweaks - can these go in a .csv lookup file?

```{r}
library(hera)
library(tidyverse)
```

# Test Locations

Only surveillance sites for speed of testing.

```{r}
locs <- read.csv("vignettes//data//invert-surv-locs-subset.csv")
```

# LIMS Data

Open files of pre-fetched data from LIMS. Data was downloaded from LIMS using 'RESULTS' query. Additionally, test \> test number and sample \> collection date (DUE DATE) columns were added for data processing reasons.

## Ecology

```{r}
lims_eco <- readr::read_csv(
  "vignettes//data//2023-04-14-lims-data.csv"
)

# test_eco <- readr::read_csv(
#   "vignettes//data//2023-04-02-lims-data.csv"
# )
# 
# unique(test_eco$SAMPLING_POINT[!test_eco$SAMPLING_POINT %in% lims_eco$location_id])

```

Use collection date if sampled date not present

```{r}
lims_eco$SAMPLED_DATE[is.na(lims_eco$SAMPLED_DATE)] <-
lims_eco$DUE_DATE[is.na(lims_eco$SAMPLED_DATE)]
```

Convert LIMS data into standard data format for hera package

```{r}
lims_eco <- hera::convert(lims_eco,
                      convert_to = "hera",
                      convert_from = "sepa_lims")
```

## Chemistry

```{r}
lims_chem <- readr::read_csv(
  "vignettes//data//chem-lims-data.csv"
)

# Need a test number for convert() function but not used for chemistry so dummy
# test number added.
lims_chem$TEST_NUMBER <- 1
lims_chem <- hera::convert(lims_chem,
                      convert_to = "hera",
                      convert_from = "sepa_lims")
```

## Merge

```{r}
lims_data <- bind_rows(lims_eco, lims_chem)
```

# Recovered data

## Query

Get Ecology analytical results based on LIMS locations. WARNING this takes 1hr + to run! Better to filter to a few specific locations.

```{r, eval=FALSE}

recovered_data <- get_data(location_id = unique(lims_eco$location_id))

# chem_test <- get_data(7856, dataset = "chem_analytical_results")
# recovered_data  <- get_data(location_id = c("398593",   "8207"))

recovered_data$parameter[is.na(recovered_data$parameter)] <-
recovered_data$analysis_repname[is.na(recovered_data$parameter)]

recovered_data <- dplyr::filter(recovered_data, parameter %in% c(
  "River Family Inverts",
  "River Diatoms",
  "Invert Physical Data")
  )

write.csv(recovered_data, file = "vignettes//data//2023-04-04-data.csv")
```

## Pre-fetched data

```{r}

# Recovered data  ----------------------
eco_data <- readr::read_csv(
  "vignettes//data//2023-04-04-data.csv"
)

new_eco_data <- readr::read_csv(
  "vignettes//data//2023-04-14-data.csv"
)


eco_data <- bind_rows(eco_data, new_eco_data)

eco_data$parameter[is.na(eco_data$parameter)] <- eco_data$analysis_repname[is.na(eco_data$parameter)]

eco_data <- dplyr::filter(eco_data, parameter %in% c(
  "River Family Inverts",
  "River Diatoms",
  "SURVEY_INV")
  )

eco_data$parameter[eco_data$parameter =="SURVEY_INV"] <-  "River Family Inverts"

 eco_data$question[eco_data$question ==
                         "% Boulders/Cobbles"] <-  "boulders_cobbles"
    eco_data$question[eco_data$question ==
                         "% Pebbles/Gravel"] <-  "pebbles_gravel"
    eco_data$question[eco_data$question ==
                         "% Sand"] <- "sand"
    eco_data$question[eco_data$question ==
                         "% Silt/Clay"] <- "silt_clay"
    eco_data$question[eco_data$question ==
                         "River Width (m)"] <- "river_width"
    eco_data$question[eco_data$question ==
                         "Mean Depth (cm)"] <- "mean_depth"


alk_data <- readr::read_csv(
  "vignettes//data//alk-data.csv"
)

eco_data$date_taken <- as.Date(eco_data$date_taken)
eco_data$determinand_code <- as.character(eco_data$determinand_code)
eco_data$test_number <- as.character(eco_data$test_number)
eco_data$location_id <- as.character(eco_data$location_id)
eco_data$sample_id <- as.character(eco_data$sample_id)

 alk_data$units <- as.character(alk_data$units)
 alk_data$response <- as.character(alk_data$response)
 alk_data$test_number <- as.character(alk_data$test_number)
alk_data$location_id <- as.character(alk_data$location_id)
 alk_data$sample_id <- as.character(alk_data$sample_id)

recovered_data <- bind_rows(eco_data, alk_data)

```

# Combine Data

```{r}
data <- bind_rows(recovered_data, lims_data)
data$parameter[is.na(data$parameter)] <- "PAC"

recovered_data <- NULL
eco_data <- NULL
lims_chem <- NULL
lims_data <- NULL
lims_eco <- NULL
alk_data <- NULL
```

# Filter

Only core surveillance sites to speed up testing and filter out ecology old
samples.

```{r}
#  data <- data[data$location_id %in% unique(locs$Loc), ]
data$year <- lubridate::year(data$date_taken)
data <- data %>%  filter(year >= 2017 | parameter == "PAC")
```

# NMP

Get the NMP spreadsheets, to get the waterbody_id. This is needed to link results to previous classification outputs??...not sure - needs to be location based classification?

# Predictors

Get RICT predictors.

```{r, message=FALSE}
# Filter data-----------------------------------------------------------------
# We only want samples that pass our criteria (season, number of samples etc)
# data <- filter(data, parameter %in% c("River Family Inverts", 
#                                       "River Diatoms"))

predictors <- utils::read.csv(system.file("extdat",
     "predictors.csv",
     package = "hera"
   ),
   stringsAsFactors = FALSE, check.names = FALSE
   )
   predictors$location_id <- as.character(predictors$location_id)

   # use latest 'version' for each location_id
 predictors <- arrange(predictors, desc(date))
  predictors <- select(predictors,
                       alkalinity,
                       location_id,
                       grid_reference,
                       chemistry_site,
                       altitude,
                       slope,
                       discharge_category,
                       dist_from_source
                       )
 predictors <- predictors[!duplicated(predictors$location_id), ]

 data <- left_join(data, predictors, by = c("location_id"))

```

# Calculate Alkalinity

If alkalinity required calculate.

```{r, eval=FALSE}

data$alkalinity <- NULL
Sys.time()
alkalinity <- hera:::mean_alkalinity(data)
Sys.time()
data <- inner_join(data, 
                    alkalinity,
                    by = join_by("sample_id" == "sample_number"))
 eco_missing_alk <- data$sample_id[data$question == "pebbles_gravel" &
                                     is.na(data$alkalinity)]
 data <- data[!data$sample_id %in% eco_missing_alk, ]
 eco_missing_alk2 <- data$sample_id[data$question == "Taxon name" &
                                      is.na(data$alkalinity)]
  data <- data[!data$sample_id %in% eco_missing_alk2, ]
```

# Classification 2022

RUn 2022 classification

## Classify

```{r, message=FALSE}
data <- data[!is.na(data$grid_reference), ]

# Issue with NGR out of temp grid 
# test_data <- data[data$location_id == "134730",] 

# test_data <- data[data$location_id == "101932",] 

results <- map_df(c(2022), function(class_year){
 filtered_data <- data %>%
 mutate(year = lubridate::year(date_taken)) %>%
 filter(year <= class_year) %>%
 hera:::filter_samples(classification_year_data = FALSE)

 wfd_results <- map_df(split(filtered_data, filtered_data$location_id), function(location) {
  wfd_results <- assess(location,
                      name = c("DARLEQ3", "RICT"))
  }, .progress = TRUE)
})
```


# Filter

```{r, message=FALSE}
# Filter out the predicted values and sample level results. We only want the
# location level results

results_filtered <- filter(
  results,
  question %in% c(
    "CoCH",
    "CoCG",
    "CoCM",
    "CoCP",
    "CoCB",
    "Class",
    "EQR",
    "Years included",
    "Comments",
    "Suit Code",
    "Suit Text"
  ),
  is.na(sample_id)
) %>%
  select(-sample_id)

# Pivot results to make them look nice
results_filtered <- pivot_wider(results_filtered, 
                                names_from = question,
                                values_from = response)


results_filtered <- results_filtered %>% 
  mutate(parameter = ifelse(parameter == "Macroinvertebrates (ASPT)",
                                     "Macroinvertebrates (ASPT) - Rivers",
                                     parameter),
                   parameter = ifelse(parameter == "Macroinvertebrates (NTAXA)",
                                     "Macroinvertebrates (NTAXA) - Rivers", 
                                     parameter),
                   parameter = ifelse(parameter == "Phytobenthos (diatoms)",
                                     "Phytobenthos (diatoms) - Rivers", 
                                     parameter))

```

# NMP

Add NMP to get WBIDs?
```{r}

nmp_diatoms <- readxl::read_xlsx(path =
                                 "vignettes//data//nmp-diatoms-draft-2023.xlsx"
 )
nmp_diatoms$analysis <- "DIATOMS"

nmp_inverts <- readxl::read_xlsx(path =
                                 "vignettes//data//nmp-inverts-draft-2023.xlsx"
 )
nmp_inverts$analysis <- "INVERTS"
nmp <- bind_rows(nmp_diatoms, nmp_inverts)

```

# Join NMP

```{r}
nmp <- select(nmp, "location_id" = Loc,
              "wbid" = WBID,
              analysis,
              "replocs_check" = `CHK REPLOCS`,
              "mon_purp" = `REG Mon Reason`
              )

diatoms <- nmp[nmp$analysis == "DIATOMS",]
diatoms$analysis <- "Phytobenthos (diatoms) - Rivers"
inverts <- nmp[nmp$analysis == "INVERTS",]
aspt <- inverts
ntaxa <- inverts
aspt$analysis <- "Macroinvertebrates (ASPT) - Rivers"
ntaxa$analysis <-  "Macroinvertebrates (NTAXA) - Rivers"
all_nmp <- bind_rows(aspt, ntaxa, diatoms)
all_nmp$location_id <- as.character(all_nmp$location_id)
all_nmp$wbid <- as.character(all_nmp$wbid)
results_filtered <- left_join(results_filtered, all_nmp, by = join_by(location_id,
                                                  parameter == analysis))


```


# Previous Class

Join 2020 classification results based on WB
```{r}

aspt_class <- readxl::read_excel(path = 
   "vignettes//data//aspt-classification-2020.xlsx")
aspt_class$parameter <- "Macroinvertebrates (ASPT) - Rivers"

ntaxa_class <- readxl::read_excel(path =
    "vignettes/data//ntaxa-classification-2020.xlsx")
ntaxa_class$parameter <- "Macroinvertebrates (NTAXA) - Rivers"

diatom_class <- readxl::read_excel(path =
    "vignettes//data//diatom-classification-2020.xlsx")
diatom_class$parameter <- "Phytobenthos (diatoms) - Rivers"

previous_class <- bind_rows(aspt_class, ntaxa_class, diatom_class)

previous_class$WBID <- as.character(previous_class$WBID)
results_filtered <- left_join(results_filtered, previous_class, by = join_by(wbid == WBID, parameter == parameter))

results_filtered$change <- results_filtered$Class == results_filtered$CONDITION
results_filtered$change[results_filtered$change == FALSE] <- "Y"
results_filtered$change[results_filtered$change == TRUE] <- "N"

test <- results_filtered[!is.na(results_filtered$change), ]
test$Class <- dplyr::recode_factor(test$Class,"High" = "High", "Good" = "Good", "Moderate" = "Moderate")
test$CONDITION <- dplyr::recode_factor(test$CONDITION, "High" = "High", "Good" = "Good", "Moderate" = "Moderate")
diatom_test <- test[test$parameter == "Phytobenthos (diatoms) - Rivers", ]
invert_test <- test[test$parameter == "Macroinvertebrates (ASPT) - Rivers", ]
table(diatom_test$Class, diatom_test$CONDITION)
table(invert_test$Class, invert_test$CONDITION)
```


# Check Years

2022 can include results from with data only from 2019. Only include locations
with data from 2021 or 2022.

```{r}
results_filtered$`Years included` <- 
  gsub(",", "&", results_filtered$`Years included`)
results_filtered$check <- 
  grepl("2021", results_filtered$`Years included`)

results_filtered$check[results_filtered$check == FALSE] <- 
   grepl("2022", results_filtered$`Years included`[results_filtered$check == FALSE])

results_filtered <- results_filtered[results_filtered$check == TRUE, ]
```

# Select
```{r}
results_filtered <- results_filtered %>%  select(
  "Parameter" = parameter,
  "WB ID" = wbid,
  "Location Code" = location_id,
  EQR,
  Class,
  CoCH,
  CoCG,
  CoCM,
  CoCP,
  CoCB,
  change,
  `Years included`,
  `Suit Code`,
  `Suit Text`,
  "Previous class" = CONDITION,
  mon_purp,
  replocs_check
)


```

# Reportable

```{r}

results_filtered$wfd_reportable <-  grepl("[SM]", results_filtered$mon_purp)
results_filtered$wfd_reportable[results_filtered$wfd_reportable == TRUE | results_filtered$replocs_check == "YES"] <- "TRUE"

```


# Save

```{r}
write.csv(results_filtered, file = "vignettes\\data\\2022-classification.csv", row.names = FALSE)

write.csv(data, file = "vignettes\\data\\2022-data.csv", row.names = FALSE)


```




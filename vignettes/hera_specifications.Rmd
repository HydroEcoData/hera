---
title: "Request for Comment: Joint Platform for Aquatic Research and Deployment"
date: "`r Sys.Date()`"
pkgdown:
  as_is: yes
output: 
  rmarkdown::html_vignette:
    number_sections: yes
    toc: yes
vignette: >
  %\VignetteIndexEntry{Request for Comment: Joint Platform for Aquatic Research and Deployment}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}  
editor_options: 
  chunk_output_type: console 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(comment = "")
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

library(tidyverse)
library(hera)
library(darleq3)
library(fcs2)
library(rict)

```

```{r results="asis", echo=FALSE}
# directly adding css to output html without ruining css style https://stackoverflow.com/questions/29291633/adding-custom-css-tags-to-an-rmarkdown-html-document
cat("
<style>
img {
border: 0px;
outline: 0 ;
}
</style>
")
```
WORK IN PROGRESS - Drafting

![](images/heraverse_logo_2.png){ width=60% }

WARNING: Blue sky thinking ahead

Keywords: collection, modelling, prediction, classification, forecast, scenario

# Summary

We propose all new tool development be added to a collection of R
packages which share the same design philosophy, interface and data structures.
  
This RFC proposes the creation of 'hera' - an official collection of R packages
designed to provide collaborative workflow for building and using classification
tools. This facilitates faster iteration and improved integration and knowledge
exchange between method developers and practitioners.

# Motivation

UKTAG has lead the development of an impressive range of classification tools.
This has involved many developers, researchers and experts dedicating their time
and effort to creating tools to better understand pressures on the environment.
We are confident that there are many future opportunities for collaboration and
tool development in response to changing environmental pressures and improving
scientific understanding. For instance, new tools for investigate diagnostic
work, updates to existing tools and catchment scale planning.

The aims of a shared design philosophy for WFD classification R packages include:
  
- Create a single user interfaces, similar data formats and operating procedures.
- Allow outputs from multiple tools to be quickly generated and combined together.
- Shorten the time between development and integration into Agencies systems. 
- Where possible, define common routes to accessing and sharing of reference data and predictor values.
- Make it easier to share and re-use code for confidence, uncertainty and model suitability across current or future methods.
- Share data quality standards and data validation code. 
- Apply similar approaches to code review, testing and documentation.

# Key Ideas
  
- Clearer path for researcher engagement and model develop.
- Provide common APIs / web services to allow agencies to
integrate into existing databases.
- Allowing researchers/consultancies to use the R packages directly or through a GUI.

## Preparing for change

In the next 10 years...
  
The ease and access to modelling tools means there is a proliferation of models,
indices and metrics. We have seen many useful results from modelling techniques
in weather forecasting, flood planning and climate change research. It is likely
that our aquatic ecological models will be subsumed into a more large scale
environmental and climate models. These 'total environment' models may for
instance use climate change models to forecast impact on invertebrates,
water-use models to predict the impact on fish or flooding models the impact of
macrophytes. These large scale environmental models could integrate elements of
existing invertebrate, fish and macrophytes models and reference data into large
hybrid models also incorporating rainfall, flow and climate predictions. The
outputs from which we be used across disciplines to plan flood management,
biodiversity improvements, carbon sequestering. Allowing multi-disciplines
assessment of impacts and trade-offs for each planning scenario, ensuring
informed decisions-making.

All ecology data will be freely accessible and bundled together (along with
chemistry, climate, meteorological, geological etc) and multiple models run
across the same data, providing outputs on many aspects of ecology, chemistry,
climate, flooding, biodiversity etc.

We assume Ecologist will upload fish counter data, plant DNA or aerial imagery,
which will be ingested into the 'lake' of environmental data. A huge number of
models and predictions automatically run based on this new data providing fresh
insights in environmental harms, biodiversity trends, pressures and impacts.
Regular summary reports (largely drafted automatically) provide new findings and
forecasts to be discussed and addressed.

To make step towards this vision, the underlying design of a models and tools
must be modular and easy to connect and integrate.

# Detailed design

To aid collaboration and to response to the changing environmentally pressures,
we propose creating a joint research platform to share understanding on the
environment while providing the software infrastructure to lighten the burden of
more mundane tasks involved in maintaining and deploying new models and
interfaces. 

This is influenced by the work of climate change research and weather
forecasting communities. And on-going work in the R community such as [ropenSci](https://ropensci.org/).

## Groups

All UKTAG sub-groups and their nominated leads in the devolved agencies would
contribute new method develops and tools to the shared collection of packages.
Where tools are agency specific, these could also make use of the platform if
required.

## Organisation

Currently some model packages are shared on github for instance
[darleq3](https://github.com/nsj3/darleq3),
[fsc2](https://github.com/aquaMetrics/fcs2),
[rict](https://github.com/aquaMetrics/rict). We propose these tools are shared
under a single package  once the tool is finalised.
  
As agencies commission new tools to be developed, researchers can upload their
predictive variables, reference data and models into a central repo for easier
collaboration.

## Lots of datasets - one underlying data structure?

Ecological modelling relies on sampling. The samples come in a range of forms
from points, transects, images, grabs, DNA etc. But the general feature of
modelling is based on being able to predict what we expect to find from whatever
sampling technique we deploy. The sample is the fundamental observation which we
compare against our prediction. The samples are discreet and independent, either
observed instantaneously or perhaps over a few minutes or hours (where dynamic
changes are not significant).
  
Multiple samples can be aggregated to smooth variance but the sample still
remains the fundamental building block. The sample could be a single pixel from
an aerial image or a salmon moving through a fish counter. We still make
predictions of what we expect this sample to be like even if the true picture
only emerges after several samples are aggregated or compared.

## What does this look like?

Below is an example of diatom records, invert data and river flow in a shared
input format.
  
They share some reference/book keeping variables but not all. Ultimately, only
one reference is needed which is a unique sample id. The other reference
variables can be 'nested' or in others words there can be as many or a few as
you like. These nested values could be sample type, collector, instrument
details etc. Or variables later used for aggregation such as water body, river,
geographic area etc. For example:

```{r}
data <- utils::read.csv(system.file("extdat",
                                    "test-data.csv",
                                    package = "hera"
))


knitr::kable(data.frame("sample_id" = 192342,
                        "nested meta data" = names(data[, 2:6]),
                        "question" = "whpt_aspt_abundance",
                        "response" = 6.348,
                        check.names = FALSE))


```

## Predictors

Predictive variables such as temperature, altitude, slope etc are also metadata
can can be nested, as they are the same for each sample.

## Observations 

The observation comes in two parts, the unique name/id for what you are
observing and the value associated with it. For clarity these variables are
called 'question' and 'response'. The question could be "Taxon name?" and the
response "Brown Trout". However, the question must be unique. As 'Taxon name?'
will be different depending on what survey is being undertaken. So question_id
and response_id are given to allow these to be unique with metadata to provide a
more readable format.
  
UUID are used to provide unique IDs for this variables. 

```{r}

 data <- nest(data, meta_variables = c(2:6, 9:26))
# data <- nest(meta_variables = c(2:6))
knitr::kable(data[1:2,])

```

## Data input

Data input is through mobile or internet connected devices. The question_id and
related meta data is configured. Manual or automated data can be collected.
As new WFD developments and updates requirements are identified, the lead
contacts from the agencies and method developers are 'on-boarded' to explain the
expectations and examples of building collaborative framework of packages. Where
skill develop is required further training can be provided, or additional
external or internal support from the agency commissioning the work.
A workshop for lead data experts / R coders from each agency delivers
institutional knowledge on how internally developed tools will fit with the
design philosophy and expectations of how external researchers will collaborate
on building new tools.
## Model platform

There is no prescribed modelling program or software. Researchers can download
the data provided and use any software they desire - as long as it has an API

Alternatively, if researchers can't provide an api for others. The
recommendation is to use R - which integrates more directly into the pipeline.

Once modelling is completed, the model object is saved and deployed. Any
existing or future data collected using the platform will be run through the
model at the sample level.

Researchers can then build tools to display and aggregate the sample level
results as required (Waterbody, Year, Catchment etc). Where it would be possible
to share techniques for producing Confidence of Class, assessment of data
suitability and adjustment factors etc.

## hera

Here we introduce a prototype package called hera. hera provides a framework for
taking in ecological data, detecting what type of data has been entered, running
the relevant models and returning a classification. The examples below are based
on 'fake' data, indices and classification methods and is just 'dream code' at
the moment.

This demo dataset contains information on diatoms, fish and inverts:

```
library(hera)
str(demo_data)

```

We run this dataset through hera providing a prediction for each taxa in each
sample:

```

predictions <- hera_predict(demo_data)
head(predictions)

```

Based on an associated score for each taxa, we predict indices for each sample:

```

indices <- hera_indices(demo_data)
head(indices)

```

Based on the predicted and observed indices - we classify each sample:

```

class <- hera_classify(demo_data)
head(class)

```

We now aggregated the samples by season, year or multi-year as required:

```

aggregate <- hera_aggregate(demo_data)
head(class)

```

We now run a report assesses samples from the same location for consistency.
Note, that no report for fish is produced, the necessary adjustment/intreptation
parameters have not been created for fish. This demostrates that it is step by
step process. Not all tools/models will have all the features development if not
required or if not prioritized:

```
compare_report <- compare(demo_data)
compare_report

```

We now run a report assesses two samples assess a discharge (up and downstream):  

```
compare_report <- compare(demo_data)
compare_report

```
## Sharing

hera allows multiple ecological elements to be assessed through the same
interface. But not just the interface is shared. Other areas of the
infrastructure are shared including:

- Reporting and comparison tools can also be shared between multiple
elements. 
- Validation checks and other universally required mechanisms
are shared and easily configurable for new models/methods.
- Testing infrastructure. 
- Confidence of class and data suitability algorithms. 

# How we Teach This

As new WFD developments and updates requirements are identified, the lead
contacts from the agencies and method developers are 'on-boarded' to explain the
expectations and examples of building collaborative framework of packages. Where
skill develop is required further training can be provided, or additional
external or internal support from the agency commissioning the work.
  
A workshop for lead data experts / R coders from each agency delivers
institutional knowledge on how internally developed tools will fit with the
design philosophy and expectations of how external researchers will collaborate
on building new tools.

# Alternatives



# Unresolved questions

- Should all reference data be combined into a single repository / web service?
- Should all predictor data be combined into a single repository / web service?

# Appendix


## Input formats

### FCS2 tool 

Demo input data format (truncated) and full list of column names
```{r}
test <- head(demo_data[, 1:5], 4)
test$... <- "..."
test
row.names(test) <- NULL
names(demo_data)
```

### Darleq tool

Input data for DARLEQ3 tool is a list of dataframes. Here's an example of input
data format (truncated) and full list of column names

```{r}

file <- system.file("extdata/DARLEQ2TestData.xlsx", package="darleq3")
data <- read_DARLEQ(file, "Rivers TDI Test Data")
test <- data$diatom_data[1:4, 1:8]
test$... <- "..."
test
names(data$diatom_data)
names(data$header)
```

### RICT

Here's an example of input data format (truncated) and full list of column names

```{r}
test <- rict::demo_observed_values[1:4, 1:8]
test$... <- "..."
test
names(demo_observed_values)
```

## Input formats

### FCS2 example

```{r, include=FALSE}
results <- calcClassScot(data = fcs2::demo_data)
```

```{r}
test <- results[1:4, 1:6]
test$... <- "..."
test
names(results)
```

### RICT example

```{r, warning=F, message=F}
test <- rict(demo_ni_observed_values)
example <- head(test[1: 6], 4)
example$... <- "..."
example
names(test)
```

### Darleq data

(list of dataframes)

```{r}
fn <- system.file("extdata/DARLEQ2TestData.xlsx", package="darleq3")
d <- read_DARLEQ(fn, "Rivers TDI Test Data")
results <- calc_Metric_EQR(d, metrics=c("TDI4", "TDI5LM"))
head(results$TDI5LM$EQR[, 9:13])
head(results$TDI5LM$Uncertainty[, 9:13])
head(results$TDI5LM$Metric)
head(results$TDI5LM$Job_Summary, 4)
```




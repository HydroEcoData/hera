---
title: "Assessment Template"
output: rmarkdown::html_vignette
editor_options: 
  chunk_output_type: console
runtime: shiny
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = F
)
library(hera)
library(tidyverse)
library(testthat)
```

## Welcome

This document has been created following the generic assessment guidance.


## Details

<!-- Create a new regulatory assessment by changing the CAPITLIZE info below: -->

```{r setup}

standard <- tibble(
  standard_short = NA, 
  quality_element = NA,
  parameter = "Test Assessment", 
  standard_long = NA, 
  standard_reference = NA,
  aggregation = NA,
  status = NA
)

standard_format <- hera:::hera_format(standard = standard) # format assessment data
standard_format$standard %>%
  filter(attribute != "standard_reference") %>%
  knitr::kable()
```

## Data

```{r data}
data <- tibble(
  parameter = NA,
  question = NA,
  response = NA,
  label = NA,
  sample_id = NA,
  location_id = NA,
  location_description = NA,
  grid_reference = NA,
  water_body_id = NA,
  year = NA,
  latitude = 0,
  longitude = 0,
  date_taken = NA,
  units = NA, 
  min = NA, 
  max = NA
)
data
```

## Locate

Metadata required about the location

```{r}
# A list of metadata for location of sampling
location <- data %>% select(
  location_id, # optional (recommended)
  location_description, 
  grid_reference
)
location <- location %>% unique()
head(location)
```

## Sample

Metadata required to assess each sample. (Excluding direct observations)

```{r}
# A list of questions and responses collected through observation
sample <- data %>% select(
  parameter,
  sample_id, # optional (recommended)
  date_taken
)
sample <- sample %>% unique()
head(sample)
```

## Observe

```{r}
questions <- data %>% select(
  question,
  response,
  label # optional (usually for taxonomic/species name)
)
questions
```


## Metrics

If applicable...

```{r indices}

indices_function <- function(data) {
  # Some calculated index e.g. summary of sample responses: 
  # index = sum(data$response, na.rm = TRUE)
  index <- NA

  data <- tibble(
    predictor_1 = NA,
    predictor_2 = NA,
    sample_id = NA
  )
  return(data)
}
indexes <- indices_function(data)
indexes
```

## Predictors

```{r predictors}
predictors <- tibble(
   PREDICTOR_1 = NA,
   PREDICTOR_2 = NA,
   location_id = NA
  # ...
)
knitr::kable(predictors) 
```

## Predict

```{r prediction}
prediction_function <- function(predictors) {

  # Generates prediction based on predictors...
  prediction <- tibble(
    question = NA,
    response = NA,
    location_id = NA
  )

  return(prediction)
}
predictions <- prediction_function(predictors)
predictions
```

## Assessment

```{r assessment_table}

assessment_table <- tibble(
  asesssment = c( "high", 
                  "good", 
                  "moderate",
                  "poor",
                  "bad"),
  value = c(0.80,
            0.60,
            0.40,
            0.20,
            0)
  # ...
  # "red" = 0.33
  # "amber" = 0.5
  # "green" = 0.66
  
  # "pass" = 0.5
  # "fail" = 0.0
)

assessment_table
```

## Assess

```{r assessment}
assessment_function <- function(data, assessment_table) {

  # Compares observation against prediction...
  # usually: eqr <- prediction / data$response
  
  
  eqr <- 1 - predictions$response / data$response
  
  # Final EQR cut! ------------------------------------------------------------
  class <- cut(eqr,
             breaks = c(1, assessment_table$value),
             labels = assessment_table$asesssment)
  
  
   assessments <- tibble(
    sample_id = NA,
    question = NA,
    response = class
  )
  
  return(assessments)
}
assessments <- assessment_function(predictions, data, assessment_table)
assessments
```


## Confidence

Confidence of assessment. 

```{r}

confidence_function <- function(data, aggregates = "sample_id") {

  confidence <- tibble(
    aggregates = NA,
    question = NA,
    response = NA
  )
  
  names(confidence)[1] <- aggregates[1]
  
}

```

## Checklist

```{r checklist}
check_list <- hera:::hera_test(standard = standard)
knitr::kable(check_list$standard_check)
```

## Update

```{r hera_update}
# No need to edit this code
model_dataframe <- hera::model_dataframe
model <- tibble(
  analysis_name = standard$parameter,
  standard = list(standard),
  location = list(location[1, ]),
  sample = list(sample[1, ]),
  validation_function = NA,
  indices_function = list(indices_function),
  prediction_function = list(prediction_function),
  assessment_function = list(assessment_function),
  confidence_function = list(confidence_function),
  indices = list(indexes[indexes$sample_id == indexes$sample_id[1], ]),
  assessment_table = list(assessment_table),
  questions = list(questions[1, ]),
  predictors = list(predictors[1, ]),
  predictions = list(predictions[predictions$location_id == predictions$location_id[1], ])
)

model_dataframe <- model_dataframe[model_dataframe$analysis_name != standard$parameter, ]

model_dataframe <- bind_rows(model_dataframe, model)
new_model_dataframe <- model_dataframe
new_model_dataframe 
usethis::use_data(model_dataframe, overwrite = TRUE)
```

## Launch app

Below is an interactive application displaying the results of your assessment.

```{r launch_app, echo=FALSE}
# No need to edit this code
launch_app(new_model_dataframe = model_dataframe, data = data)

```
